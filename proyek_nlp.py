# -*- coding: utf-8 -*-
"""Proyek NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19NRf7waqPdJRattdVQvuPaC-nmWPUbqe
"""

# multiclass text classification
from google.colab import drive
drive.mount('/content/drive/')

# load dataset
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/dicoding/review_film.csv')

df

import tensorflow as tf

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.95):
      print("\nTraning data berhenti, tingkat akurasi > 95%")
      self.model.stop_training = True

callbacks = myCallback()

# encoding
cat = pd.get_dummies(df.sentiment)
df = pd.concat([df, cat], axis = 1)
df = df.drop(columns = 'sentiment')

df

# split dataframe
review = df['text'].values
label = df[['neg', 'pos']].values

review

label

from sklearn.model_selection import train_test_split

text = df['text'].values
y = df[['neg', 'pos']].values
text_train, text_test, y_train, y_test = train_test_split(text, y, test_size = 0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# tokenizer
tokenizer = Tokenizer(num_words = 280617, oov_token = '-')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

# sequences
seq_train = tokenizer.texts_to_sequences(text_train)
seq_test = tokenizer.texts_to_sequences(text_test)

# padding
pad_train = pad_sequences(seq_train, maxlen = 300, padding = 'post', truncating = 'post')

pad_test = pad_sequences(seq_test, maxlen = 300, padding = 'post', truncating = 'post')

# embedding
from tensorflow.keras import layers
from tensorflow.keras import Sequential

model = Sequential([layers.Embedding(280617, 64, input_length = 300),
                    layers.LSTM(64, dropout = 0.1),
                    layers.Dense(128, activation = 'relu'),
                    layers.Dense(64, activation = 'relu'),
                    layers.Dense(2, activation = 'sigmoid')
                    ])

model.summary()

# model training
model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

history = model.fit(pad_train, 
                    y_train, 
                    batch_size = 128, 
                    epochs = 30,
                    validation_data = (pad_test, y_test),
                    verbose = 2,
                    callbacks = [callbacks])

# model accuracy
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

# model loss
import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()